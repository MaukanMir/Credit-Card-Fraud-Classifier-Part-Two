{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Models \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Data Processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# MLP\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Model Processing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spot_check_models(X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded):\n",
    "    \"\"\"\n",
    "    This a quick way to spot check relevant algorithms to gain an understanding of \n",
    "    the dataset and which models handle the distribution well.\n",
    "\n",
    "    Args:\n",
    "        X_train_scaled (_type_): _description_\n",
    "        X_test_scaled (_type_): _description_\n",
    "        y_train_encoded (_type_): _description_\n",
    "        y_test_encoded (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Sorted dataframe on accuracy scores.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        \"LDA\":LinearDiscriminantAnalysis(),\n",
    "        \"GPC\":GaussianProcessClassifier(),\n",
    "        \n",
    "        \"LogisticRegression\": LogisticRegression(),\n",
    "        \"SVC\": SVC(),\n",
    "        \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "        \n",
    "        \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(random_state=42),\n",
    "        \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=42),\n",
    "        \"XGB\":XGBClassifier()\n",
    "    }\n",
    "\n",
    "    # Create an empty DataFrame to store model performance\n",
    "    model_performance = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_encoded)\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test_encoded, predictions)\n",
    "        model_performance.append({\n",
    "            \"Model\": name,\n",
    "            \"Accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "    # For the Sequential model\n",
    "    sequential_model = Sequential()\n",
    "    sequential_model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "    sequential_model.add(Dense(32, activation='relu'))\n",
    "    sequential_model.add(Dense(1, activation='sigmoid'))\n",
    "    sequential_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    sequential_model.fit(X_train_scaled, y_train_encoded, epochs=50, batch_size=10, verbose=0)\n",
    "    loss, accuracy = sequential_model.evaluate(X_test_scaled, y_test_encoded)\n",
    "    model_performance.append({\n",
    "        \"Model\": \"Sequential\",\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "    # Convert the model_performance to a DataFrame\n",
    "    performance_df = pd.DataFrame(model_performance)\n",
    "    return performance_df.sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "\n",
    "def get_selected_models(names):\n",
    "  \"\"\"\n",
    "  Returns selected models for ML processing\n",
    "\n",
    "  Args:\n",
    "      names (_type_):List\n",
    "\n",
    "  Returns:\n",
    "      List of models\n",
    "  \"\"\"\n",
    "  models = {\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"GPC\": GaussianProcessClassifier(),\n",
    "    \"GNB\": GaussianNB(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"LR\":LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DTC\": DecisionTreeClassifier(),\n",
    "    \"GBC\":GradientBoostingClassifier(),\n",
    "    \"RFC\":RandomForestClassifier(),\n",
    "    \"XGB\": XGBClassifier()\n",
    "  }\n",
    "  \n",
    "  return [models[model] for model in names]\n",
    "\n",
    "def evaluate_model(X, y, model, metric):\n",
    "  # define evaluation procedure\n",
    "  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "  # evaluate model\n",
    "  scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "  return scores\n",
    "\n",
    "def labels_to_probabilities(y):\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return probabilities\n",
    "\n",
    "def calculate_entropy(df:pd.DataFrame)-> pd.DataFrame:\n",
    "  \"\"\"_summary_\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): Pandas DataFrame\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: THe Entropy level of all models\n",
    "  \"\"\"\n",
    "\n",
    "  column_entropy_info = {}\n",
    "  for col in df.columns:\n",
    "    probabilities = labels_to_probabilities(df[col])\n",
    "    entropy_value = entropy(probabilities, base=2)\n",
    "    column_entropy_info[col] = {\n",
    "          'entropy': entropy_value\n",
    "      }\n",
    "\n",
    "  return pd.DataFrame(column_entropy_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in a subset of the data and try to match the class imbalance in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "total_samples = 10000\n",
    "ratio = 258  \n",
    "\n",
    "\n",
    "positives_needed = total_samples // (ratio + 1)\n",
    "negatives_needed = total_samples - positives_needed\n",
    "\n",
    "\n",
    "positives_count = 0\n",
    "negatives_count = 0\n",
    "\n",
    "\n",
    "sampled_data = []\n",
    "\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "# Randomly sample from each chunk\n",
    "for chunk in pd.read_csv('data.csv', chunksize=chunk_size):\n",
    "    # Separate positive and negative cases\n",
    "    positives = chunk[chunk['is_fraud'] == 1]\n",
    "    negatives = chunk[chunk['is_fraud'] == 0]\n",
    "\n",
    "    positives_sample = positives.sample(min(len(positives), positives_needed - positives_count))\n",
    "    negatives_sample = negatives.sample(min(len(negatives), negatives_needed - negatives_count))\n",
    "\n",
    "    positives_count += len(positives_sample)\n",
    "    negatives_count += len(negatives_sample)\n",
    "\n",
    "    sampled_data.append(positives_sample)\n",
    "    sampled_data.append(negatives_sample)\n",
    "\n",
    "    # Break if we have enough samples\n",
    "    if positives_count >= positives_needed and negatives_count >= negatives_needed:\n",
    "        break\n",
    "\n",
    "# Concatenate all sampled data into a single DataFrame\n",
    "final_sample = pd.concat(sampled_data, ignore_index=True)\n",
    "\n",
    "# Shuffle the final sample to mix positive and negative cases\n",
    "df = final_sample.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               0\n",
       "trans_date_trans_time    0\n",
       "cc_num                   0\n",
       "merchant                 0\n",
       "category                 0\n",
       "amt                      0\n",
       "first                    0\n",
       "last                     0\n",
       "gender                   0\n",
       "street                   0\n",
       "city                     0\n",
       "state                    0\n",
       "zip                      0\n",
       "lat                      0\n",
       "long                     0\n",
       "city_pop                 0\n",
       "job                      0\n",
       "dob                      0\n",
       "trans_num                0\n",
       "unix_time                0\n",
       "merch_lat                0\n",
       "merch_long               0\n",
       "is_fraud                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, Count=9962, Percentage=99.620%\n",
      "Class=1, Count=38, Percentage=0.380%\n"
     ]
    }
   ],
   "source": [
    "target = df.values[:,-1]\n",
    "\n",
    "counter = Counter(target)\n",
    "\n",
    "for k, v in counter.items():\n",
    "  per = v/len(target) * 100\n",
    "  print(\"Class=%d, Count=%d, Percentage=%.3f%%\" % (k,v,per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important lesson for ML engineers, the class balance is 258:1 for the original dataset, in our sample it is 262:1. This means For every 258 examples of the majoirty class, there is only one from the minority class. Posting a 99% accuracy score shows a misunderstanding of the problem at hand. Because there are so few fraud examples, the model will not be able to predict the minority class which is costly to the business.\n",
    "\n",
    "## Business Problem:\n",
    "\n",
    "### Fraud cases are costly to banks, false negatives will be far more costly to their bottome line than false positives.\n",
    "\n",
    "* Accuracy and F1 score's are irrelevant in this instance\n",
    "* F1 score's and accuracy scores do not penalize false negatives\n",
    "* F2 score's are what should be measured since they give greater weight to false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 int64\n",
       "trans_date_trans_time     object\n",
       "cc_num                   float64\n",
       "merchant                  object\n",
       "category                  object\n",
       "amt                      float64\n",
       "first                     object\n",
       "last                      object\n",
       "gender                    object\n",
       "street                    object\n",
       "city                      object\n",
       "state                     object\n",
       "zip                        int64\n",
       "lat                      float64\n",
       "long                     float64\n",
       "city_pop                   int64\n",
       "job                       object\n",
       "dob                       object\n",
       "trans_num                 object\n",
       "unix_time                  int64\n",
       "merch_lat                float64\n",
       "merch_long               float64\n",
       "is_fraud                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = df.select_dtypes(include=[\"float64\",\"int64\"])\n",
    "categorical_features = df.select_dtypes(include=\"object\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
